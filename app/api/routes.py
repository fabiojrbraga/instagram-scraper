"""
Endpoints da API REST.
Define as rotas para scraping, consulta de dados, etc.
"""

import logging
from typing import Any
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from sqlalchemy.orm.attributes import flag_modified
from datetime import datetime
from urllib.parse import urlparse

from app.database import get_db
from app.schemas import (
    ScrapingJobCreate,
    ScrapingJobResponse,
    ScrapingCompleteResponse,
    ProfileScrapeRequest,
    ProfileScrapeResponse,
    ProfileResponse,
    PostResponse,
    InteractionResponse,
    ErrorResponse,
)
from app.models import Profile, Post, Interaction, ScrapingJob
from app.scraper.instagram_scraper import instagram_scraper

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api", tags=["instagram"])


def _extract_instagram_username(profile_url: str) -> str:
    """Extrai username de URL completa ou valor simples."""
    value = (profile_url or "").strip()
    if not value:
        return ""
    if not value.startswith(("http://", "https://")):
        return value.strip("/").split("/")[0].strip()

    parsed = urlparse(value)
    if "instagram.com" not in parsed.netloc.lower():
        return ""
    path_parts = [part for part in parsed.path.split("/") if part]
    return path_parts[0].strip() if path_parts else ""


def _normalize_profile_url(profile_url: str) -> str:
    """Normaliza URL para formato can√¥nico do Instagram."""
    username = _extract_instagram_username(profile_url)
    if username:
        return f"https://www.instagram.com/{username}/"
    return (profile_url or "").strip()


def _safe_int(value: Any, default: int = 0) -> int:
    try:
        return int(value)
    except (TypeError, ValueError):
        return default


# ==================== Health Check ====================

@router.get("/health")
async def health_check():
    """Verifica sa√∫de da aplica√ß√£o."""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
    }


# ==================== Scraping Endpoints ====================

@router.post("/scrape", response_model=ScrapingJobResponse)
async def start_scraping(
    request: ScrapingJobCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
):
    """
    Inicia um job de scraping de um perfil Instagram.

    Args:
        request: URL do perfil a raspar
        background_tasks: Para executar scraping em background
        db: Sess√£o do banco de dados

    Returns:
        Informa√ß√µes do job criado
    """
    try:
        normalized_profile_url = _normalize_profile_url(request.profile_url)
        logger.info(
            "üì• Requisi√ß√£o de scraping recebida: %s (normalizado: %s)",
            request.profile_url,
            normalized_profile_url,
        )

        # Criar job de scraping
        request_payload = request.model_dump(mode="json", exclude_unset=True)
        request_payload["profile_url"] = normalized_profile_url
        job = ScrapingJob(
            profile_url=normalized_profile_url,
            status="pending",
            metadata_json={"request": request_payload},
        )
        db.add(job)
        db.commit()
        db.refresh(job)

        # Executar scraping em background
        background_tasks.add_task(
            _scrape_profile_background,
            job_id=job.id,
            profile_url=normalized_profile_url,
            options={k: v for k, v in request_payload.items() if k != "profile_url"},
        )

        logger.info(f"‚úÖ Job de scraping criado: {job.id}")

        return ScrapingJobResponse(
            id=job.id,
            profile_url=job.profile_url,
            status=job.status,
            created_at=job.created_at,
        )

    except Exception as e:
        logger.error(f"‚ùå Erro ao criar job de scraping: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/scrape/{job_id}", response_model=ScrapingJobResponse)
async def get_scraping_status(
    job_id: str,
    db: Session = Depends(get_db),
):
    """
    Obt√©m status de um job de scraping.

    Args:
        job_id: ID do job
        db: Sess√£o do banco de dados

    Returns:
        Status do job
    """
    try:
        job = db.query(ScrapingJob).filter(ScrapingJob.id == job_id).first()

        if not job:
            raise HTTPException(status_code=404, detail="Job n√£o encontrado")

        return ScrapingJobResponse(
            id=job.id,
            profile_url=job.profile_url,
            status=job.status,
            started_at=job.started_at,
            completed_at=job.completed_at,
            error_message=job.error_message,
            posts_scraped=job.posts_scraped,
            interactions_scraped=job.interactions_scraped,
            created_at=job.created_at,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter status do job: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/scrape/{job_id}/results", response_model=ScrapingCompleteResponse)
async def get_scraping_results(
    job_id: str,
    db: Session = Depends(get_db),
):
    """
    Obt√©m resultados completos de um job de scraping.

    Args:
        job_id: ID do job
        db: Sess√£o do banco de dados

    Returns:
        Resultados do scraping
    """
    try:
        job = db.query(ScrapingJob).filter(ScrapingJob.id == job_id).first()

        if not job:
            raise HTTPException(status_code=404, detail="Job n√£o encontrado")

        if job.status != "completed":
            raise HTTPException(
                status_code=400,
                detail=f"Job ainda n√£o foi conclu√≠do. Status: {job.status}",
            )

        metadata = job.metadata_json or {}
        request_payload = metadata.get("request", {}) if isinstance(metadata.get("request"), dict) else {}
        flow = metadata.get("flow") or request_payload.get("flow")
        flow_result = metadata.get("result")

        if flow == "recent_likes" and isinstance(flow_result, dict):
            posts = flow_result.get("posts", []) or []
            summary = flow_result.get("summary", {}) or {}
            profile_payload = flow_result.get("profile", {}) or {}
            return ScrapingCompleteResponse(
                job_id=job.id,
                status=job.status,
                flow="recent_likes",
                profile={
                    "username": profile_payload.get("username") or "",
                    "profile_url": profile_payload.get("profile_url") or job.profile_url,
                    "bio": profile_payload.get("bio"),
                    "is_private": bool(profile_payload.get("is_private", False)),
                    "follower_count": profile_payload.get("follower_count"),
                    "posts": [
                        {
                            "post_url": post.get("post_url", ""),
                            "caption": post.get("caption"),
                            "like_count": _safe_int(post.get("like_count", 0) or 0),
                            "comment_count": _safe_int(post.get("comment_count", 0) or 0),
                            "interactions": [],
                        }
                        for post in posts
                        if post.get("post_url")
                    ],
                },
                extracted_posts=posts,
                total_posts=_safe_int(summary.get("total_posts", len(posts)) or 0),
                total_interactions=_safe_int(summary.get("total_like_users", 0) or 0),
                raw_result=flow_result,
                error_message=job.error_message,
                completed_at=job.completed_at,
            )

        # Buscar perfil associado
        profile = db.query(Profile).filter(
            Profile.instagram_url == job.profile_url
        ).first()

        if not profile:
            normalized_job_url = _normalize_profile_url(job.profile_url)
            if normalized_job_url and normalized_job_url != job.profile_url:
                profile = db.query(Profile).filter(
                    Profile.instagram_url == normalized_job_url
                ).first()

        if not profile:
            username = _extract_instagram_username(job.profile_url)
            if username:
                profile = db.query(Profile).filter(
                    Profile.instagram_username == username
                ).first()

        if not profile and isinstance(flow_result, dict):
            posts = flow_result.get("posts", []) or []
            interactions = flow_result.get("interactions", []) or []
            summary = flow_result.get("summary", {}) or {}
            profile_payload = flow_result.get("profile", {}) or {}
            return ScrapingCompleteResponse(
                job_id=job.id,
                status=job.status,
                flow=flow or "default",
                profile={
                    "username": profile_payload.get("username") or _extract_instagram_username(job.profile_url),
                    "profile_url": profile_payload.get("profile_url") or _normalize_profile_url(job.profile_url),
                    "bio": profile_payload.get("bio"),
                    "is_private": bool(profile_payload.get("is_private", False)),
                    "follower_count": profile_payload.get("follower_count"),
                    "posts": [
                        {
                            "post_url": post.get("post_url", ""),
                            "caption": post.get("caption"),
                            "like_count": _safe_int(post.get("like_count", 0) or 0),
                            "comment_count": _safe_int(post.get("comment_count", 0) or 0),
                            "interactions": [],
                        }
                        for post in posts
                        if post.get("post_url")
                    ],
                },
                total_posts=_safe_int(summary.get("total_posts", len(posts)) or 0),
                total_interactions=_safe_int(summary.get("total_interactions", len(interactions)) or 0),
                raw_result=flow_result,
                error_message=job.error_message,
                completed_at=job.completed_at,
            )

        if not profile:
            logger.warning(
                "Perfil n√£o encontrado para job %s; retornando resultado baseado no metadata/job sem consulta de perfil.",
                job.id,
            )
            return ScrapingCompleteResponse(
                job_id=job.id,
                status=job.status,
                flow=flow or "default",
                profile={
                    "username": _extract_instagram_username(job.profile_url),
                    "profile_url": _normalize_profile_url(job.profile_url),
                    "bio": None,
                    "is_private": False,
                    "follower_count": None,
                    "posts": [],
                },
                total_posts=_safe_int(job.posts_scraped, 0),
                total_interactions=_safe_int(job.interactions_scraped, 0),
                raw_result=flow_result if isinstance(flow_result, dict) else None,
                error_message=job.error_message,
                completed_at=job.completed_at,
            )

        # Buscar posts e intera√ß√µes
        posts = db.query(Post).filter(Post.profile_id == profile.id).all()
        interactions = db.query(Interaction).filter(
            Interaction.profile_id == profile.id
        ).all()

        # Montar resposta
        result = ScrapingCompleteResponse(
            job_id=job.id,
            status=job.status,
            flow="default",
            profile={
                "username": profile.instagram_username,
                "profile_url": profile.instagram_url,
                "bio": profile.bio,
                "is_private": profile.is_private,
                "follower_count": profile.follower_count,
                "posts": [
                    {
                        "post_url": post.post_url,
                        "caption": post.caption,
                        "like_count": post.like_count,
                        "comment_count": post.comment_count,
                        "interactions": [
                            {
                                "type": interaction.interaction_type.value,
                                "user_url": interaction.user_url,
                                "user_username": interaction.user_username,
                                "user_bio": interaction.user_bio,
                                "is_private": interaction.user_is_private,
                                "comment_text": interaction.comment_text,
                            }
                            for interaction in interactions
                            if interaction.post_id == post.id
                        ],
                    }
                    for post in posts
                ],
            },
            total_posts=len(posts),
            total_interactions=len(interactions),
            error_message=job.error_message,
            completed_at=job.completed_at,
        )

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter resultados do scraping: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Profile Endpoints ====================

@router.post("/profiles/scrape", response_model=ProfileScrapeResponse)
async def scrape_profile_info(
    request: ProfileScrapeRequest,
    db: Session = Depends(get_db),
):
    """
    Raspa um perfil espec√≠fico e retorna somente dados do perfil.
    """
    try:
        normalized_profile_url = _normalize_profile_url(request.profile_url)
        logger.info(
            "üì• Requisi√ß√£o de scrape de perfil recebida: %s (normalizado: %s)",
            request.profile_url,
            normalized_profile_url,
        )
        result = await instagram_scraper.scrape_profile_info(
            profile_url=normalized_profile_url,
            db=db if request.save_to_db else None,
            save_to_db=request.save_to_db,
        )
        return ProfileScrapeResponse(**result)
    except Exception as e:
        logger.error("‚ùå Erro ao raspar perfil %s: %s", request.profile_url, e)
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/profiles/{username}", response_model=ProfileResponse)
async def get_profile(
    username: str,
    db: Session = Depends(get_db),
):
    """
    Obt√©m informa√ß√µes de um perfil.

    Args:
        username: Username do perfil
        db: Sess√£o do banco de dados

    Returns:
        Informa√ß√µes do perfil
    """
    try:
        profile = db.query(Profile).filter(
            Profile.instagram_username == username
        ).first()

        if not profile:
            raise HTTPException(status_code=404, detail="Perfil n√£o encontrado")

        return ProfileResponse.from_orm(profile)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter perfil: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/profiles/{username}/posts")
async def get_profile_posts(
    username: str,
    skip: int = 0,
    limit: int = 10,
    db: Session = Depends(get_db),
):
    """
    Obt√©m posts de um perfil.

    Args:
        username: Username do perfil
        skip: N√∫mero de posts a pular
        limit: N√∫mero m√°ximo de posts a retornar
        db: Sess√£o do banco de dados

    Returns:
        Lista de posts
    """
    try:
        profile = db.query(Profile).filter(
            Profile.instagram_username == username
        ).first()

        if not profile:
            raise HTTPException(status_code=404, detail="Perfil n√£o encontrado")

        posts = db.query(Post).filter(
            Post.profile_id == profile.id
        ).offset(skip).limit(limit).all()

        return {
            "username": username,
            "total": len(posts),
            "posts": [PostResponse.from_orm(post) for post in posts],
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter posts do perfil: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/profiles/{username}/interactions")
async def get_profile_interactions(
    username: str,
    skip: int = 0,
    limit: int = 50,
    db: Session = Depends(get_db),
):
    """
    Obt√©m intera√ß√µes de um perfil.

    Args:
        username: Username do perfil
        skip: N√∫mero de intera√ß√µes a pular
        limit: N√∫mero m√°ximo de intera√ß√µes a retornar
        db: Sess√£o do banco de dados

    Returns:
        Lista de intera√ß√µes
    """
    try:
        profile = db.query(Profile).filter(
            Profile.instagram_username == username
        ).first()

        if not profile:
            raise HTTPException(status_code=404, detail="Perfil n√£o encontrado")

        interactions = db.query(Interaction).filter(
            Interaction.profile_id == profile.id
        ).offset(skip).limit(limit).all()

        return {
            "username": username,
            "total": len(interactions),
            "interactions": [
                InteractionResponse.from_orm(interaction)
                for interaction in interactions
            ],
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter intera√ß√µes do perfil: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Background Tasks ====================

async def _scrape_profile_background(job_id: str, profile_url: str, options: dict | None = None):
    """
    Executa scraping em background.

    Args:
        job_id: ID do job
        profile_url: URL do perfil a raspar
        options: op√ß√µes avan√ßadas do fluxo
    """
    db = None
    try:
        db = next(get_db())

        # Atualizar status do job
        job = db.query(ScrapingJob).filter(ScrapingJob.id == job_id).first()
        if not job:
            logger.error(f"Job n√£o encontrado: {job_id}")
            return

        job.status = "running"
        job.started_at = datetime.utcnow()
        db.commit()

        opts = dict(options or {})
        flow = (opts.get("flow") or "default").lower().strip()
        default_max_posts = 3 if flow == "recent_likes" else 5
        max_posts = int(opts.get("max_posts", default_max_posts))
        recent_hours = int(opts.get("recent_hours", 24))
        max_like_users_per_post = int(opts.get("max_like_users_per_post", 30))
        # Etapa de enriquecimento de perfis curtidores foi removida do /scrape.
        # Mesmo que venha true na request, for√ßamos false neste job.
        collect_like_user_profiles = False

        # Executar scraping de acordo com o fluxo.
        if flow == "recent_likes":
            result = await instagram_scraper.scrape_recent_posts_like_users(
                profile_url=profile_url,
                max_posts=max_posts,
                recent_hours=recent_hours,
                max_like_users_per_post=max_like_users_per_post,
                collect_like_user_profiles=collect_like_user_profiles,
                db=db,
            )
        else:
            result = await instagram_scraper.scrape_profile(
                profile_url=profile_url,
                max_posts=max_posts,
                db=db,
            )

        # Atualizar job com resultados
        job.status = "completed"
        job.completed_at = datetime.utcnow()
        summary = result.get("summary", {}) if isinstance(result, dict) else {}
        total_posts = int(summary.get("total_posts", 0) or 0)
        if flow == "recent_likes":
            total_interactions = int(summary.get("total_like_users", 0) or 0)
        else:
            total_interactions = int(summary.get("total_interactions", 0) or 0)

        job.posts_scraped = total_posts
        job.interactions_scraped = total_interactions
        base_metadata = job.metadata_json if isinstance(job.metadata_json, dict) else {}
        metadata = dict(base_metadata)
        metadata["flow"] = flow
        metadata["options"] = dict(opts)
        metadata["result"] = result
        job.metadata_json = metadata
        flag_modified(job, "metadata_json")
        db.commit()

        logger.info(f"‚úÖ Job conclu√≠do: {job_id}")

    except Exception as e:
        logger.exception("‚ùå Erro no scraping em background: %s", e)

        if db:
            job = db.query(ScrapingJob).filter(ScrapingJob.id == job_id).first()
            if job:
                job.status = "failed"
                job.error_message = str(e)
                job.completed_at = datetime.utcnow()
                db.commit()

    finally:
        if db:
            db.close()
